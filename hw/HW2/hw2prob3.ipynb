{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "hw2prob3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_TWKuKgsIpS",
        "colab_type": "text"
      },
      "source": [
        "# Neural Networks for Musical Instrument Classification\n",
        "\n",
        "In this assignment, we will attempt a musical instrument classification problem.  Given a sample of music, we want to determine which instrument (e.g. trumpet, violin, piano) is playing.  \n",
        "\n",
        "*This assignment is closely based on one by Sundeep Rangan, from his [IntroML GitHub repo](https://github.com/sdrangan/introml/).*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6oKkg9TsIpl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlHuh8JJsIpr",
        "colab_type": "text"
      },
      "source": [
        "## Audio Feature Extraction with Librosa\n",
        "\n",
        "The key to audio classification is to extract useful features. The `librosa` package in Python has a rich set of methods for extracting the features of audio samples commonly used in machine learning tasks, such as speech recognition and sound classification. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzYcyXLAsIpt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "import librosa.feature"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ax6MvKnQsIp0",
        "colab_type": "text"
      },
      "source": [
        "In this lab, we will use a set of music samples from the website:\n",
        "\n",
        "http://theremin.music.uiowa.edu\n",
        "\n",
        "We will use the `wget` command to retrieve one file to our Google Colab storage area. (We can run `wget` and many other basic Linux commands in Colab by prefixing them with a `!` or `%`.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DWD96V_sIp1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget \"http://theremin.music.uiowa.edu/sound files/MIS/Woodwinds/sopranosaxophone/SopSax.Vib.pp.C6Eb6.aiff\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1aEv46-1js5",
        "colab_type": "text"
      },
      "source": [
        "Now, if you click on the small folder icon on the far left of the Colab interface, you can see the files in your Colab storage. You should see the \"SopSax.Vib.pp.C6Eb6.aiff\" file appear there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXO-9u212Tcj",
        "colab_type": "text"
      },
      "source": [
        "In order to listen to this file, we'll first convert it into the `wav` format. Again, we'll use the `!` to run a basic command-line utility: `ffmpeg`, a powerful tool for working with audio and video files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRIGtdJt2IiH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aiff_file = 'SopSax.Vib.pp.C6Eb6.aiff'\n",
        "wav_file = 'SopSax.Vib.pp.C6Eb6.wav'\n",
        "\n",
        "!ffmpeg -y -i $aiff_file $wav_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SNKADBT1hwE",
        "colab_type": "text"
      },
      "source": [
        "Now, we can play the file directly from Colab. If you listen to it you will hear a soprano saxaphone (with vibrato) playing four notes (C, C#, D, Eb)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTGS1qsM0ivn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import IPython.display as ipd\n",
        "ipd.Audio(wav_file) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXk7GRz3sIp6",
        "colab_type": "text"
      },
      "source": [
        "Next, use `librosa` command `librosa.load` to read the audio file with filename `audio_file` and get the samples `y` and sample rate `sr`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YQpJxgQsIp7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y, sr = librosa.load(aiff_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF4s4DMSsIqE",
        "colab_type": "text"
      },
      "source": [
        "Feature engineering from audio files is an entire course on its own right.  A commonly used set of features are called the Mel Frequency Cepstral Coefficients (MFCCs).  These are derived from the so-called mel spectrogram, which extracts features that correlate with human audio perception.  \n",
        "\n",
        "You can run the code below to display the mel spectrogram from the audio sample.\n",
        "\n",
        "You can easily see the four notes played in the audio track.  You also see the 'harmonics' of each notes, which are other tones at integer multiples of the fundamental frequency of each note."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6cOsDyvsIqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)\n",
        "librosa.display.specshow(librosa.amplitude_to_db(S),\n",
        "                         y_axis='mel', fmax=8000, x_axis='time')\n",
        "plt.colorbar(format='%+2.0f dB')\n",
        "plt.title('Mel spectrogram')\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3uHiRLksIqR",
        "colab_type": "text"
      },
      "source": [
        "## Downloading the Data\n",
        "\n",
        "\n",
        "Using the MFCC features described above, [Prof. Juan Bello](http://steinhardt.nyu.edu/faculty/Juan_Pablo_Bello) and his former PhD student Eric Humphrey have created a complete data set that can used for instrument classification.  Essentially, they collected a number of data files from the website above.  For each audio file, the segmented the track into notes and then extracted 120 MFCCs for each note.  The goal is to recognize the instrument from the 120 MFCCs.  The process of feature extraction is quite involved.  So, we will just use their processed data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZlpRZxp30l6",
        "colab_type": "text"
      },
      "source": [
        "To retrieve their data, visit\n",
        "\n",
        "https://github.com/marl/dl4mir-tutorial/blob/master/README.md\n",
        "\n",
        "and note the password listed on that page. Click on the link for \"Instrument Dataset\", enter the password, click on `instrument_dataset` to open the folder, and download the four files there. and note the password listed on that page. Click on the link for \"Instrument Dataset\", enter the password, click on `instrument_dataset` to open the folder, and download the four files there. (You can \"direct download\" straight from this site, you don't need a Dropbox account.)\n",
        "\n",
        "\n",
        "Then, upload the files to your Google Colab storage: click on the folder icon on the left to see your storage, if it isn't already open, and then click on \"Upload\". Wait until _all_ uploads have completed.\n",
        "\n",
        "Then, load the files with:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-MTepSYsIqT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Xtr = np.load('uiowa_train_data.npy')\n",
        "ytr = np.load('uiowa_train_labels.npy')\n",
        "Xts = np.load('uiowa_test_data.npy')\n",
        "yts = np.load('uiowa_test_labels.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3trTmlFisIqb",
        "colab_type": "text"
      },
      "source": [
        "Examine the data you have just loaded in:\n",
        "\n",
        "* What are the number of training and test samples?\n",
        "* What is the number of features for each sample?\n",
        "* How many classes (i.e. instruments) are there?\n",
        "\n",
        "Write some code to find these values and print them.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7Z-2mD3sIqc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO 1 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWWizQMlsIqn",
        "colab_type": "text"
      },
      "source": [
        "Then, standardize the training and test data, `Xtr` and `Xts`, by removing the mean of each feature and scaling to unit variance. \n",
        "\n",
        "\n",
        "You can do this manually, or using `sklearn`'s [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). \n",
        "\n",
        "Make sure you standardize both the training and test data using the mean and variance of the *training data only*.  (If using a `StandardScaler`: create a single `StandardScaler`, call `fit` with the training data, then call `tranform` with the training data, and finally call `transform` with the test data.)\n",
        "\n",
        "<small>Standardizing input data can make the gradient descent easier; see [this video](https://www.youtube.com/watch?reload=9&v=UIp2CMI0748) for further explanation.</small>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UvWuLuNsIqp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO 2 Scale the training and test matrices\n",
        "# Xtr_scale = ...\n",
        "# Xts_scale = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "SNuUMo6tsIqy",
        "colab_type": "text"
      },
      "source": [
        "## Building a Neural Network Classifier\n",
        "\n",
        "Following the example in the [demo you have seen](https://colab.research.google.com/drive/1t2OeBGcfB5HSDFl6FPQFaQKbmeEAPPgG?usp=sharing), prepare and create a neural network with the following configuration:\n",
        "\n",
        "* 256 hidden units in a single dense hidden layer\n",
        "* sigmoid activation at hidden units\n",
        "* `softmax` activation at the output (since this is a multi-class classification problem)\n",
        "* Cross-entropy loss\n",
        "* Adam optimizer with a learning rate of 0.001\n",
        "* print the model summary\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBJSHZHzsIrA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO 3 construct the model, print model summary, and compile the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6KptU6fsIra",
        "colab_type": "text"
      },
      "source": [
        "Fit the model for 10 epochs (passes through the entire data). Use the scaled training data to fit the model, and also pass the test data as \"validation data\" so that the loss and accuracy will be computed on the test data as well.\n",
        "\n",
        "Use a batch size of 128.  Your final accuracy should be >99%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "0lVnI-udsIrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO 4 fit the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dbq7u992sIrk",
        "colab_type": "text"
      },
      "source": [
        "Plot the training and test accuracy vs. epochs on one subplot, and the training and test loss vs. epoch on another subplot.  Use a log scale for the vertical axis on the loss plot.\n",
        "\n",
        "You should see that the test accuracy saturates at a little higher than 99%.  After that it may \"bounce around\" due to the noise in the stochastic mini-batch gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "magsqASCsIrl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO 5 two subplots: one of accuracy vs. epochs, one of loss vs. epochs\n",
        "# in each subplot, show training in one color and test in another color"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8celLnivsIrw",
        "colab_type": "text"
      },
      "source": [
        "## Varying the Learning Rate\n",
        "\n",
        "One challenge in training neural networks is the selection of the learning rate.  Rerun the above code, trying four learning rates as shown in the vector `rates`.  For each learning rate, \n",
        "\n",
        "* clear the session\n",
        "* prepare a neural network model as described above, with the appropriate learning rate\n",
        "* train the model for 20 epochs\n",
        "* save the accuracy and losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3DcP3aTsIry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rates = [0.1, 0.01,0.001,0.0001]\n",
        "\n",
        "# TODO 6\n",
        "for lr in rates:\n",
        "        # do stuff here...\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNhXQawFsIr5",
        "colab_type": "text"
      },
      "source": [
        "Plot the training loss vs. the epoch for all of the learning rates on one plot.  You should see that the lower learning rates are more stable, but converge slower, while with a learning rate that is too high, the gradient descent may fail to move towards weights that decrease the loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1ONYE8rsIr6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO 7 one plot showing training loss vs. epoch\n",
        "# use a different color for each learning rate"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}